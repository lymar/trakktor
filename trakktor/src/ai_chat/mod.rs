use std::sync::Arc;

use anyhow::Context;
use chat_doc::{ChatDoc, Msg};
use clap::Parser;

use crate::{
    llm::{ChatCompletionPlatform, ChatCompletionsArgs, Message},
    open_ai::OpenAiAPI,
};

pub mod chat_doc;

#[derive(Parser, Debug)]
pub struct AIChat {
    /// Specify the file containing the chat messages to be processed.
    #[arg(long, short)]
    pub file: std::path::PathBuf,
    /// The configuration to use for processing.
    #[arg(long, short)]
    pub cfg_name: Option<Arc<str>>,
    /// Ignores and overwrites the last message if it was generated by the AI
    /// (assistant).
    #[arg(long, short, default_value_t = false)]
    pub overwrite_last_response: bool,
}

pub struct AllChatProviders {
    pub open_ai: OpenAiAPI,
}

pub async fn run_ai_chat(
    ai_chat: &AIChat,
    chat_platform: &Option<ChatCompletionPlatform>,
    chat_model: &Option<Arc<str>>,
    all_providers: &AllChatProviders,
) -> anyhow::Result<()> {
    let mut doc = ChatDoc::load(&ai_chat.file).await?;

    if ai_chat.overwrite_last_response {
        // The last message is deleted only if it was in the main file, i.e., it
        // is not from include.
        if let Some(true) =
            doc.original_chat_data.msgs.last().map(Msg::is_assistant)
        {
            doc.msgs.pop();
            let msgs = doc.toml_doc["msgs"].as_array_of_tables_mut().unwrap();
            msgs.remove(msgs.len() - 1);
        }
    }

    let doc_configs = &doc.original_chat_data.cfg;
    let mut config = if let Some(cfg_name) = &ai_chat.cfg_name {
        // Find the configuration by name.
        doc_configs
            .iter()
            .find(|cfg| cfg.name.as_deref() == Some(cfg_name))
            .cloned()
            .ok_or_else(|| {
                anyhow::anyhow!("Configuration '{cfg_name}' not found")
            })?
    } else {
        // Use the first configuration if no name is specified.
        // If there are no configurations, use the default.
        doc_configs.get(0).map(|v| v.clone()).unwrap_or_default()
    };

    if config.platform.is_none() {
        // If the provider is not specified in the configuration, use the
        // command line argument.
        if chat_platform.is_none() {
            anyhow::bail!("Chat Provider not specified");
        } else {
            config.platform = chat_platform.clone();
        }
    }

    if config.model.is_none() {
        // If the model is not specified in the configuration, use the command
        // line argument.
        config.model = chat_model.as_ref().map(|s| s.to_string());
    }

    tracing::debug!("Using configuration: {config:#?}");

    let messages = doc
        .msgs
        .iter()
        .map(|msg| match msg {
            Msg::Text { role, content } => Ok(Message {
                role: *role,
                content: content.into(),
            }),
            Msg::Include { .. } => {
                Err(anyhow::anyhow!("Unexpected include message"))
            },
        })
        .collect::<anyhow::Result<Vec<_>>>()?;

    let response_format = if let Some(format) = &config.response_format {
        Some(serde_json::from_str(format).with_context(|| {
            format!("Failed to parse the response format as JSON: {format}")
        })?)
    } else {
        None
    };

    let chat = ChatCompletionsArgs::builder()
        .maybe_model_overwrite(config.model.as_deref())
        .messages(&messages)
        .maybe_response_format(response_format.as_ref())
        .build();

    let chat_msg = match config
        .platform
        .ok_or_else(|| anyhow::anyhow!("Chat Platform not specified"))?
    {
        ChatCompletionPlatform::OpenAI => {
            chat.run_with(&all_providers.open_ai).await?
        },
    };

    let mut msg = Msg::Text {
        role: chat_msg.role,
        content: chat_msg.content.to_string(),
    };

    if let (true, Msg::Text { content, .. }) =
        (config.beautify_json_response, &mut msg)
    {
        match serde_json::from_str::<serde_json::Value>(content) {
            Ok(s) => {
                *content = serde_json::to_string_pretty(&s)?;
            },
            Err(_) => {
                tracing::warn!("Failed to beautify JSON response");
            },
        }
    }

    let value = serde::Serialize::serialize(
        &msg,
        toml_edit::ser::ValueSerializer::new(),
    )
    .unwrap();

    let msgs = doc.toml_doc["msgs"].as_array_of_tables_mut().unwrap();
    msgs.push(value.as_inline_table().unwrap().clone().into_table());

    doc.write_doc(&ai_chat.file).await?;

    Ok(())
}
